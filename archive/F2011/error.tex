\documentclass[11pt]{amsart}
\usepackage[top=1.0in,bottom=1.0in,left=1.0in,right=1.0in]{geometry}
\title{Error computations using partial derivatives}
\author{Dave Rosoff}
\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}

Experiments are often concerned with the indirect measurement of some quantity, say $z$, by means of the direct measurement of other quantities $x_1, x_2, \ldots, x_n$ and the judicious use of relationships among the $x_j$ and between $z$ and the $x_j$. The easiest case to discuss is that of the relationship
\[
z = f(x_1, \ldots, x_n),
\]
that is, where there are no dependencies among the $x_j$ and a single relationship (the function $f$) between $z$ and the collection of $x_j$. Let us take $n = 2$; the general case is similar.

Suppose now that $z = f(x,y)$ and that we are attempting the measurement of $z$ by the direct, physical measurement of $x$ and $y$. Think of $x$ and $y$ as constants whose value is unchanging, yet can only be known approximately (this is the typical situation known as reality). If we measure these two numbers, using our apparatus, we obtain approximations
\[
\tilde{x} \approx x, \qquad \tilde{y} \approx y.
\]
I choose the notation so that the relationship between $\tilde{x}$ and $x$ is evident in the symbols, and even suggested by the typographical similarity between the tilde $\tilde{\phantom{x}}$ and approximation sign $\approx$. The absolute errors in our $x$ and $y$ measurements are therefore
\[
\Delta x := \tilde{x} - x, \qquad  \Delta y := \tilde{y} - y.
\]
Here the symbol ``$:=$'' indicates a definition. The relative errors are the ratios $\dfrac{\Delta x}{x}$ and $\dfrac{\Delta y}{y}$. The percent errors are the scaled ratios $100 \dfrac{\Delta x}{x}$ and $100 \dfrac{\Delta y}{y}$, traditionally written $\dfrac{\Delta x}{x} \times 100\%$ and $\dfrac{\Delta y}{y} \times 100\%$.

How are we to estimate the relative (or equivalently, percent) error $\frac{\Delta z}{z} = \frac{\tilde{z} - z}{z} = \frac{f(\tilde{x},\tilde{y}) - f(x,y)}{z}$? It would be convenient to have an expression for $\frac{\Delta z}{z}$ in terms of the other relative errors. Measuring devices are only accurate to within certain acceptable tolerances, usually expressed as percent errors. If your Geiger counter, which measures in units of cpm (counts per minute) is accurate to within $0.5\% = 0.005$, this means the following. You cannot say with certainty how many counts (radioactive particles) passed through the detector in the observation time interval. If the detector registered $\tilde{n}$ counts, all you can say is that $\left| \frac{\Delta n}{n} \right|$ is no greater than $0.005$. Hence $\left| \tilde{n} - n \right|$ is no greater than $0.005 n$. A bit of algebra (do it!) shows that, in this case, the observed value $\tilde{n}$ lies in a definite \emph{interval}:
\[
    0.995n \leq \tilde{n} \leq 1.005 n.
\]
This is the meaning of tolerance and error in physical apparatus. Unfortunately, if the relationship expressed by $f$ is at all nontrivial, errors in measuring $x$ and $y$ will compound, contributing to larger errors in measurement of $z$, the quantity of interest. It is the responsibility of the scientist to understand how the tolerances of her apparatus determine the maximum possible error in the overall measurement. Since $\Delta x$ and $\Delta y$ are small, the linearization of $z$ is naturally of some use here.

Now, owing to our choices of notation, we are in a bit of a bind. The numbers $x$ and $y$ are the fixed, immutable, unchanging values of the constants we're measuring. They can't be strictly regarded as variables. So let's use $\tilde{x}$ and $\tilde{y}$ to stand in for them. After all, they are only observed values. In a constellation of repeated experiments many different $\tilde{x}$s and $\tilde{y}$s will be observed. So they really are variables. Now we may say that the linearization of $\tilde{z} = f(\tilde{x},\tilde{y})$ about the point $(x,y)$ is (provided $f$ is differentiable, which we gladly assume):
\[
    L(\tilde{x},\tilde{y}) = f(x,y) + \dfrac{\partial f}{\partial \tilde{x}} (\tilde{x} - x) + \dfrac{\partial f}{\partial \tilde{y}} (\tilde{y} - y)
\]
This is just the definition of the linearization $L$. Invoke the definitions of $\Delta x$ and $\Delta y$ to write
\[
    L(\tilde{x},\tilde{y}) = f(x,y) + \dfrac{\partial f}{\partial \tilde{x}} \Delta x + \dfrac{\partial f}{\partial \tilde{y}} \Delta y.
\]
Now since $f$ is locally linear at $(x,y)$, if $\Delta x$ and $\Delta y$ are small, we have $L(\tilde{x},\tilde{y}) \approx f(\tilde{x},\tilde{y})$. Hence
\begin{align}\label{eq:approx}
\Delta z &= f(\tilde{x},\tilde{y}) - f(x,y) \notag \\
         &\approx L(\tilde{x},\tilde{y}) - f(x,y) \\
         &= \dfrac{\partial f}{\partial \tilde{x}} \Delta x + \dfrac{\partial f}{\partial \tilde{y}} \Delta y. \notag
\end{align}
This is the desired relationship. Division by $z$ on both sides will, in favorable cases, relate the percent errors in $z$, $x$, and $y$.

For example, consider a pyramid with rectangular base of sides $x$ and $y$ and height $h$. The volume is known to be equal to $V = f(x,y,h) = xyh/3$. To write down an approximation for $\Delta V$ we compute the partial derivatives $f_x = yh/3$, $f_y = xh/3$, and $f_h = xy/3$. Observe also that
\[
    \frac{f_x}{V} = \frac{1}{x}, \quad \frac{f_y}{V} = \frac{1}{y}, \quad \frac{f_h}{V} = \frac{1}{h}.
\]
Equation~\ref{eq:approx} therefore gives (after dividing by $V$ so that the left-hand side gives the relative error)
\begin{align}\label{eq:example}
\frac{\Delta V}{V} &= \frac{f_x}{V} \Delta x + \frac{f_y}{V} \Delta y + \frac{f_h}{V} \Delta h \\
                   &= \frac{\Delta x}{x} + \frac{\Delta y}{y} + \frac{\Delta h}{h}
\end{align} 
In this simple relationship $V = xyh/3$, the errors compound only additively. The total (relative) error in $V$ is the sum of the errors in $x$, $y$, and $h$ separately, as Equation~\ref{eq:example} shows. Perform this procedure in the examples in the text to see what happens with more general relationships. Here is another example.

Ohm's Law tells us that the resistance in a simple resistor circuit is given by $R = E/I$, where $E$ is the voltage drop across the resistor and $I$ is the current flowing through it. Regard $R = R(E,I)$ as a function of two variables and compute the partial derivatives:
\[
    \frac{\partial R}{\partial E} = \frac{1}{I}, \qquad \frac{\partial R}{\partial I} = \frac{-E}{I^2}.
\]
Now the error-linearization formula tells us that 
\[
\Delta R \approx \frac{1}{I} \Delta E  - \frac{E}{I^2} \Delta I.
\]
The negative sign is unfortunate. There may be cancellation of error. In scientific practice we find the square root of the sum of the squares of the individual errors as a more careful upper bound. It is possible to explain these squares in terms of probability (it is the variances that add linearly, not the standard deviations which we are finding). The usual intuitive explanation is that error doesn't ``cancel'' as above when it is calculated this way:
\[
\Delta R \approx \sqrt{\frac{1}{I^2} (\Delta E)^2 + \frac{E^2}{I^4} (\Delta I)^2}.
\]
\end{document}
